<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xinqi Liu</title>
  
  <meta name="author" content="Xinqi Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpeg" href="images-our/seal_icon.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xinqi Liu | 刘 鑫 琦</name>
              </p>
              <p>Hi, this is Xinqi Liu's home page. I am currently a senior researcher at Department of Computer Vision Technology (<a href="https://vis.baidu.com/#/" target="_blank">VIS</a>), Baidu Inc. I obtained my Ph.D. degree from School of Mechanical Engineering, <a href="http://www.zju.edu.cn/english/" target="_blank">Zhejiang University </a>, advised by Prof. <a href="https://person.zju.edu.cn/lijituo" target="_blank">Jituo Li</a>.
              <p>
                My research focuses on <b>3D AIGC, 3D Reconstruction and 3D Digital Human</b>. My main goal is reconstruct robust and vivid human bodies, clothing and scenes from the real world in a low-cost way. 
                <!-- Before moving back to Zhejiang University, I received my BS degree from <a href="http://en.csu.edu.cn/" target="_blank">Central //South University of China</a> and Ph.D degree from Zhejiang University, 
                and then I was an assistant professor at Institute of Automation, <a href="http://english.cas.cn/" target="_blank">Chinese Academy of Sciences</a> . -->
              </p>
              
              <!-- <p>
                I am looking for students, please drop me an email if you are interested in working with me.
              </p> -->
              <p style="text-align:center;font-size:16px">
                Email:&nbsp;&nbsp;<a href="mailto:liuxinqi7@126.com" style="font-size:16px">liuxinqi7@126.com</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%" align=center>
              <a href="images-our/photo_my6.jpeg"><img style="width:60%;max-width:100%" alt="profile photo" src="images-our/photo_my6.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <hr/>
              <!-- <p>
                My research interests include computer graphics, computer vision and machine learning with a focus on reconstruction and human perception. 
                In addition, mannequin robot design and industrial robot control are also my main research areas.
              </p> -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="text-align: center;">
                <img src='images-our/paper12.gif' width="180" height="140">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.liuxinqi.cn" target="_blank">
                <papertitle>GVA: Reconstructing Vivid 3D Gaussian Avatars from Monocular Videos</papertitle>
              </a>
              <br>
              <strong>Xinqi Liu</strong>, 
              Chenming Wu, 
              Jialun Liu, 
              Xing Liu, 
              Jinbo Wu,
              Chen Zhao, 
              Haocheng Feng, 
              Errui Ding, 
              Jingdong
              <br>
              <em>Arxiv Preprint, 2024</em>
              <br>
              <a href="https://arxiv.org/pdf/2402.16607" target="_blank">Paper</a>,   &nbsp;  <a href="https://3d-aigc.github.io/GVA/" target="_blank">Project</a>
              <p>
                
              </p>
              <p>A 3D Gaussian human body reconstruction method that supports both body and hand driving from monocular RGB videos.
              </p>
            </td>
          </tr> 


          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="text-align: center;">
                <img src='images-our/paper11.jpeg' width="160" height="140">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.liuxinqi.cn" target="_blank">
                <papertitle>TexRO: Generating Delicate Textures of 3D Models by Recursive Optimization</papertitle>
              </a>
              <br>
              Jinbo Wu, 
              Xing Liu, 
              Chenming Wu, 
              Xiaobo Gao, 
              Jialun Liu,
              <strong>Xinqi Liu</strong>, 
              Chen Zhao, 
              Haocheng Feng, 
              Errui Ding, 
              Jingdong Wang
              <br>
              <em>Arxiv Preprint, 2024</em>
              <br>
              <a href="https://arxiv.org/pdf/2403.15009" target="_blank">Paper</a>,   &nbsp;  <a href="https://3d-aigc.github.io/TexRO/" target="_blank">Project</a>
              <p>
              </p>
              <p>A novel method for generatingdelicate textures of a known 3D mesh by optimizing its UV texture.
              </p>
            </td>
          </tr>  
              

          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="text-align: center;">
                <img src='images-our/paper10.jpeg' width="160" height="140">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.liuxinqi.cn" target="_blank">
                <papertitle>TexOct: Generating Textures of 3D Models with Octree-based Diffusion</papertitle>
              </a>
              <br>
              Jialun Liu, 
              Chenming Wu, 
              <strong>Xinqi Liu</strong>, 
              Xing Liu, 
              Jinbo Wu, 
              Haotian Peng, 
              Chen Zhao, 
              Haocheng Feng,
              Jingtuo Liu, 
              Errui Ding
              <br>
              <em>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024</em>
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_TexOct_Generating_Textures_of_3D_Models_with_Octree-based_Diffusion_CVPR_2024_paper.pdf" target="_blank">Paper</a>
              <p>
                
              </p>
              <p>A texture generative model for 3D objects based on octree structure.
              </p>
            </td>
          </tr> 

          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="text-align: center;">
                <img src='images-our/paper9.jpeg' width="160" height="160">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.liuxinqi.cn" target="_blank">
                <papertitle>Reconstructing Complex Shaped Clothing from a Single Image with Feature Stable Unsigned Distance Fields</papertitle>
              </a>
              <br>
              <strong>Xinqi Liu</strong>,
              Jituio Li,
              Guodong Lu
              <br>
              <em>IEEE Transactions on Visualization and Computer Graphics (TVCG), 2024</em>
              <br>
              <a href="https://ieeexplore.ieee.org/document/10480309" target="_blank">Paper</a>,   &nbsp;  <a href="project/ComplexCloth/index.html" target="_blank">Project</a>
              <p>
                
              </p>
              <p>A clothing reconstruction method based on an unsigned distance field, which can reconstruct clothing with complex shapes and contours from a single image.
              </p>
            </td>
          </tr> 


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="text-align: center;">
                <img src='images-our/paper4.gif' width="160" height="160">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.liuxinqi.cn" target="_blank">
                <papertitle>Learning Pose Controllable Human Reconstruction with Dynamic Implicit Fields from a Single Image</papertitle>
              </a>
              <br>
              Jituio Li,
              <strong>Xinqi Liu</strong>,
              Guodong Lu
              <br>
              <em>IEEE Transactions on Visualization and Computer Graphics (TVCG)</em>, 2024  
              <br>
              <a href="https://ieeexplore.ieee.org/document/10423797" target="_blank">Paper</a>,   &nbsp;  <a href="project/DynamicField/index.html" target="_blank">Project</a>
              <p></p>
              <p>A novel implicit representation to generate reposed reconstruction human body from a single image and target pose. 
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="text-align: center;">
                <img src='images-our/paper3.jpeg' width="160" height="160">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.liuxinqi.cn" target="_blank">
                <papertitle>Modeling Realistic Clothing from a Single Image under Normal Guide</papertitle>
              </a>
              <br>
              <strong>Xinqi Liu</strong>,
              Jituio Li,
              Guodong Lu
              <br>
              <em>IEEE Transactions on Visualization and Computer Graphics (TVCG), 2023 </em>
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/10045805" target="_blank">Paper</a>,   &nbsp;  <a href="project/NormalCloth/index.html" target="_blank">Project</a>
              <p>
                
              </p>
              <p>A robust clothing modeling method to generate a 3D clothing model with visually consistent clothing style and wrinkles distribution from a single RGB image using normal guidance.
              </p>
            </td>
          </tr> 




          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="text-align: center;">
                <img src='images-our/paper7.jpeg' width="160" height="160">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.liuxinqi.cn" target="_blank">
                <papertitle>Wrinkles Realistic Clothing Reconstruction using a combined Implicit and Explicit Method</papertitle>
              </a>
              <br>
              <strong>Xinqi Liu</strong>,
              Jituio Li,
              Guodong Lu
              <br>
              <em>Computer Aided Design (CAD), 2023 </em>
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010448523000465" target="_blank">Paper</a>,   &nbsp;  <a href="project/CombinedCloth/index.html" target="_blank">Project</a>
              <p></p>
              <p>We propose a new method that combines implicit and explicit ideas to reconstruct wrinkles realistic 3D clothing model and texture results.
              </p>
            </td>
          </tr> 


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="text-align: center;">
                <img src='images-our/paper5.jpeg' width="160" height="160">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.liuxinqi.cn" target="_blank">
                <papertitle>Generating High-fidelity Texture in RGB-D Reconstruction using Patches Density Regularization</papertitle>
              </a>
              <br>
              <strong>Xinqi Liu</strong>,
              Jituio Li,
              Guodong Lu
              <br>
              <em>Computer Aided Design (CAD), 2023 </em>
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0010448523000489" target="_blank">Paper</a>,   &nbsp; <a href="project/RealTexture/index.html" target="_blank">Project</a>
              <p></p>
              <p>A simple but effective regularization term is designed to deal with the texture patches fragmentation problem in texture mapping methods. 
              </p>
            </td>
          </tr> 


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="text-align: center;">
                <img src='images-our/paper6.jpeg' width="160" height="160">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.liuxinqi.cn" target="_blank">
                <papertitle>Robust and Automated Body and Clothing Reconstruction from a Single RGB Image</papertitle>
              </a>
              <br>
              <strong>Xinqi Liu</strong>,
              Jituio Li,
              Guodong Lu
              <br>
              <em>Computers & Graphics, 2023 </em>
              <br>
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0097849322002059" target="_blank">Paper</a>,   &nbsp;  <a href="project/MulayCloth/index.html" target="_blank">Project</a>
              <p></p>
              <p>An automatic body and clothing reconstruction method to generate the human body with style-matched and texture-realistic clothing results from a single image. 
              </p>
            </td>
          </tr> 


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images-our/paper1.jpeg' width="160" height="120">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/pii/S0010448521001354?via%3Dihub" target="_blank">
                <papertitle>Reconstruction of Colored Soft Deformable Objects Based on Self-Generated Template.</papertitle>
              </a>
              <br>
              Jituo Li, 
              <strong>Xinqi Liu</strong>, 
              Haijing Deng,
              Tianwei Wang,
              Guodong Lu,
              Jin Wang
              <br>
              <em>Computer Aided Design (CAD), 2022</em>
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S0010448521001354?via%3Dihub" target="_blank">Paper</a>,   &nbsp;  <a href="project/SelfTemplate/index.html" target="_blank">Project</a>
              <p></p>
              <p>A new method to reconstruct a deformable soft object with complete geometry and consistent texture by introducing an incremental-completion self-generated template (SGT).
              </p>
            </td>
          </tr> 



          



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" style="text-align: center;">
                <img src='images-our/paper8.jpeg' width="160" height="180">
              </div>
            </td>
            <td style="padding:30px;width:75%;vertical-align:middle">
              <a href="https://www.liuxinqi.cn" target="_blank">
                <papertitle>Improving RGB-D based 3D Reconstruction by Combining Voxels and Points</papertitle>
              </a>
              <br>
              <strong>Xinqi Liu</strong>,
              Jituio Li,
              Guodong Lu,
              Dongliang Zhang,
              Shihai Xing
              <br>
              <em>The Visual Computer, 2022 </em>
              <br>
              <a href="https://link.springer.com/article/10.1007/s00371-022-02661-5" target="_blank">Paper</a>
              <p></p>
              <p>A flexible 3D reconstruction framework that adopts a new
                data structure combining both voxels and points to significantly improve the reconstruction accuracy.
              </p>
            </td>
          </tr> 
          

          
          


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p style="text-align:center;font-size:small;">......</p>
              <br>
              <br>
              <hr>
              <!--
              <p style="text-align:center;font-size:small;">
                Thanks to <a href="https://jonbarron.info/" target="_blank">Jon Barron</a> for the website template.
              </p>-->
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
